---
title: Hello，大数据
tags:
  - 大数据
  - 教程
categories:
  - 大数据
keywords:
  - 大数据
cover: /img/post/hello.jpg
date: 2020-11-11 16:04:02
updated: 2020-11-11 16:04:06
---

{% note success simple %}
这是给公司同事培训讲解大数据入门知识时候做的准备笔记。
{% endnote %}

---

# Hello，大数据。

```shell
前言：

在进行今天的分享之前，咱们先说这样一个问题。

假设现在的项目所产生的数据量进行了指数级的增长，那么使用当前的系统架构能否满足这样一个数据量的存储，使用以及分析呢？

答案显而易见，肯定是满足不了的。

咱们当前系统不管是Server端还是数据库都仅仅满足在并发少，数据量少的前提下。

假设咱们后续有这样一个系统，每天的产生的数据量在千万甚至是亿级的时候，咱们应该如何去处理，保存以及应用这些数据呢？

带着这样的一个疑问，咱们开始今天的分享。
```

## 序

在过去的十几年中，各个领域都出现了大规模的数据增长，各类仪器、通信工具以及集成电路行业的发展也为海量数据的产生与存储提供了软件条件与硬件支持。

大数据，这一术语正是产生在全球数据爆炸式增长的背景下，用来形容庞大的数据集合。

由于大数据为挖掘隐藏价值提供了新的可能，如今工业界、研究界甚至政府部门等各行各业都对大数据这一研究领域密切关注。

小知识：

```shell
在2019年12月10日，中国信通院发布了《大数据白皮书(2019)》，这是中国信通院第四次发布大数据白皮书。
书中指出根据国际权威机构Statista的统计和预测，全球数据量在2019年约达到41ZB。1ZB大概是十万亿亿字节。

如果用标准DVD来存储41ZB的数据，并将它们一一并排放置，其长度相当于地球到月球的距离的2983倍。

根据IDC最新发布的统计数据，中国的数据产生量约占全球数据产生量的23%，美国的数据产生量占比约为21%，EMEA(欧洲、中东、非洲)的数据产生量占比约为30%，APJxC(日本和亚太)数据产生量占比约为18%，全球其他地区数据产生量占比约为8%。
```

![数据量](/img/post/数据量.jpg)

下面就由我这个菜鸡来给大家分享和简单的介绍一下我理解的大数据技术以及基本的使用。



```shell
3W法则：在每次学习接触一个新的技术的时候,要先搞明白这个技术是个什么(what),然后再考虑为什么要用(why),最后再去学习这个技术怎么用(how).

其实，这个学习法则，在不同场景的顺序或者意义也是不一样的，因人而异，因事而异，上述仅仅是我自己在学习过程中使用的。
```

## What？

先从字面看一下"大数据"是什么:

```
"大数据"就是一个体量特别大，数据类别特别多的数据集，并且这样的数据集无法用传统数据库工具对其内容进行抓取、管理和处理。
```

在国务院以国发〔2015〕50 号印发《促进大数据发展行动纲要》中提到：

```
大数据是以容量大、类型多、存取速度快、价值密度低为主要特征的数据集合，正快速发展为
对数量巨大、来源分散、格式多样的数据进行采集、存储和关联分析，从中发现新知识、创造新价值、提升新能力
的新一代信息技术和服务业态。
```

在 维克托·迈尔-舍恩伯格 的《大数据时代》一书中，作者做出了这样的解释：

```
大数据是建立在海量数据的基础之上，对未来预测的能力，大数据的核心就是预测。
```

上面这几个解释，都是从不同方面给解释了大数据是什么，但无论是哪种定义都具有一定的狭义性，换句话说就是：**大数据对于不同的人以及不同的行业来说有着不同的含义**。

对于投资人和创业者而言，大数据是个热门的融资标签。

对于消费者或者互联网所谓的“用户”来说，大数据就是尽可能地搜集跟终端消费者相关的隐私，然后进行营销。

对于我们这些开发人员来说，大数据它不是某个专业或一门编程语言，实际上它是一系列技术的组合运用。

从网上摘了这样的一个公式：

`大数据 = 编程技巧 + 数据结构和算法 + 分析能力 + 数据库技能 + 数学 + 机器学习 + NLP(自然语言处理) + OS(操作系统) + 密码学 + 并行编程`

## Why？

既然知道了什么是大数据，那么大数据有什么样的特点呢？

IBM公司曾提出了大数据的`5V`特征，分别是：

```shell
Volume（大量）、Velocity（高速）、Variety（多样）、`Veracity（真实性）`、Value（低价值密度）
```

 截至目前为止，一般在说大数据有什么特征的时候，都是基于IBM提出这`5V`特征来定义的。

![5V特征](/img/post/5v.png)

在这就简单的解释一下这`5V`特征所代表的意思吧。

```shell
1. 容量（Volume）

是指大规模的数据量，并且数据量呈持续增长趋势。目前一般指超过10T规模的数据量，但未来随着技术的进步，符合大数据标准的数据集大小也会变化。

2. 速率（Velocity）

即数据生成、流动速率快。

3. 多样性（Variety）

指是大数据包括多种不同格式和不同类型的数据。

4. 真实性（Veracity）

指数据的质量和保真性。

5. 价值（Value）

即低价值密度。随着数据量的增长，数据中有意义的信息却没有成相应比例增长。

```
既然大数据包含上述的这些个特征，那么这些特征带来的一系列的问题应该怎么解决呢？

1. 海量的数据应该如何存储？
2. 海量的数据应该拿什么计算？
3. 如何从数据中提取数据价值？

针对于以上三个问题，就衍生出了一种新的技术：**大数据技术**。

## How？

为了应对大数据的这几个特点以及带来的一系列问题，开源的大数据框架越来越多，越来越强，先列举一些常见的：

- 海量的数据应该如何存储？

```shell
文件存储：Hadoop HDFS、Tachyon(针对Spark)、KFS(国产)

NOSQL数据库：HBase、Redis、MongoDB、Cassandra
```

- 海量的数据应该拿什么计算？

```shell
离线计算：Hadoop MapReduce、Spark

流式、实时计算：Storm、Spark Streaming、S4(Yahoo!)、Flink、Heron(twitter)
```

- 如何从数据中提取数据价值？

```shell
查询分析：Hive、Impala、Pig、SparkSQL、Presto、Phoenix、Drill、Flink、Kylin

关系图谱：Janusgraph、Neo4J、HugeGraph

搜索引擎：ElasticSearch、Solr、Lunn
```

- 衍生技术

```shell
资源管理：YARN、Mesos

日志收集：Flume、Scribe、Logstash、Kibana

消息系统：Kafka、StormMQ、ZeroMQ、RabbitMQ

分布式协调服务：Zookeeper

集群管理与监控：Ambari、Ganglia、Nagios、Cloudera Manager

数据挖掘、机器学习：Mahout、Spark MLLib

数据同步：Sqoop

任务调度：Oozie
```

上面都是笼统的介绍了一下现有的一些技术框架，如果按照技术分类的话，可以列出下面这样一个表格。

- 按照技术分类

| 大数据技术分类 | 大数据技术与工具                             |
| -------------- | -------------------------------------------- |
| 基础架构支持   | 云计算平台（Apache Hadoop、OpenStark）       |
|                | 储存虚拟化、分布式存储                       |
|                | 虚拟化（VM、Docker）                         |
|                | 网络（OpenFlow）                             |
| 数据采集       | 数据总线                                     |
|                | ETL工具（flume、kafka、sqoop）               |
| 数据存储       | 分布式文件系统（HDFS、GFS）                  |
|                | 关系型数据库（Oracle、MySQL）                |
|                | Nosql数据库（HBase、Redis）                  |
|                | 关系型数据库和非关系新数据库的融合（Newsql） |
|                | 内存数据库（MemCache）                       |
| 数据计算       | 数据查询、统计与分析（MapReduce、Pig、hive） |
|                | 数据预测与挖掘（Spark、Mahout）              |
|                | 图谱处理                                     |
| 展示和交互     | 图形与报表（Hue）                            |
|                | 可视化工具（D3、Echart、MapV、谷歌地图）     |
|                | 增强现实技术（Google眼镜）                   |

上面这些基本囊括了比较主流的大数据框架，但是学习大数据技术，还需要一些前置的基础技能。

```shell
Java、Scala、Linux、Mysql、Maven | Gradle等。
```

## Haddop

这么些框架，今天也不能都给大家讲解介绍了，就挑一个不管干啥都会用到的组件介绍一下吧。

### What？

- 是什么？

Hadoop是由Apache基金会开发的分布式系统基础架构，主要解决了，海量数据的存储以及分析问题；

其实从大面上来说，Hadoop往往不是单单指的Hadoop框架而是Hadoop生态圈。基本就是上面列举的那些框架。

- 发展历史

  ![老哥](/img/post/nb.png)

  1. Lucene 框架是Doug Cutting开创的开源软件，用Java书写代码，实现与Google类似的全文搜索功能。开始的设计目标是构建一个大型的全网搜索引擎，包括网页抓取、索引、查询等功能。
  2. 随着抓取网页数量的增加，对于海量数据的场景，Lucene面对与Google同样的困难，**存储数据困难，检索速度慢**。
  3. 学习和模仿Google解决这些问题的办法 ：微型版Nutch。
  4. 2003年、2004年谷歌发表的三篇论文为该问题提供了可行的解决方案。
     - 分布式文件系统（GFS），可用于处理海量网页的存储
     - 分布式计算框架MAPREDUCE，可用于处理海量网页的索引计算问题。
     - 分布式的结构化数据存储系统**Bigtable**，用来处理海量结构化数据。
  5. 随后Google公开了部分GFS和MapReduce思想的细节，以此为基础Doug Cutting等人用了2年业余时间实现了DFS和MapReduce机制，使Nutch性能飙升
  6. 2001年年底Lucene成为Apache基金会的一个子项目；2005 年Hadoop 作为 Lucene的子项目 Nutch的一部分正式引入Apache基金会。
  7. 2006年2月被分离出来，成为一套完整独立的软件，起名为Hadoop；2008年1月，Hadoop成为Apache顶级项目。
  8. Hadoop这个名字不是一个缩写，它是一个虚构的名字。是Doug Cutting**儿子毛绒玩具象命名**；

- 版本介绍

  1. 0.x系列版本：hadoop当中最早的一个开源版本，在此基础上演变而来的1.x以及2.x的版本
  2. 1.x版本系列：hadoop版本当中的第二代开源版本，主要修复0.x版本的一些bug等
  3. 2.x版本系列：架构产生重大变化，引入了yarn平台等许多新特性。

- 发行版本

  1.免费开源版本apache：http://hadoop.apache.org/

  ```shell
  优点：拥有全世界的开源贡献者，代码更新迭代版本比较快
  缺点：版本的升级，版本的维护，版本的兼容性，版本的补丁都可能考虑不太周到
  
  apache所有软件的下载地址（包括各种历史版本）：http://archive.apache.org/dist/
  ```

  2.免费开源版本hortonWorks：https://hortonworks.com/

  ```shell
  （1）2011年成立的Hortonworks是雅虎与硅谷风投公司Benchmark Capital合资组建。
  （2）公司成立之初就吸纳了大约25名至30名专门研究Hadoop的雅虎工程师，上述工程师均在2005年开始协助雅虎开发Hadoop，贡献了Hadoop80%的代码。
  （3）雅虎工程副总裁、雅虎Hadoop开发团队负责人Eric Baldeschwieler出任Hortonworks的首席执行官。
  （4）Hortonworks的主打产品是Hortonworks Data Platform（HDP），也同样是100%开源的产品，HDP除常见的项目外还包括了Ambari，一款开源的安装和管理系统。
  （5）HCatalog，一个元数据管理系统，HCatalog现已集成到Facebook开源的Hive中。Hortonworks的Stinger开创性的极大的优化了Hive项目。Hortonworks为入门提供了一个非常好的，易于使用的沙盒。
  （6）Hortonworks开发了很多增强特性并提交至核心主干，这使得Apache Hadoop能够在包括Window Server和Windows Azure在内的Microsoft Windows平台上本地运行。
  ```

  3.软件收费版本ClouderaManager：https://www.cloudera.com/

  ```shell
  （1）2008年成立的Cloudera是最早将Hadoop商用的公司，为合作伙伴提供Hadoop的商用解决方案，主要是包括支持、咨询服务、培训。
  （2）2009年Hadoop的创始人Doug Cutting也加盟Cloudera公司。Cloudera产品主要为CDH，Cloudera Manager，Cloudera Support
  （3）CDH是Cloudera的Hadoop发行版，完全开源，比Apache Hadoop在兼容性，安全性，稳定性上有所增强。
  （4）Cloudera Manager是集群的软件分发及管理监控平台，可以在几个小时内部署好一个Hadoop集群，并对集群的节点及服务进行实时监控。Cloudera Support即是对Hadoop的技术支持。
  （5）Cloudera的标价为每年每个节点4000美元。Cloudera开发并贡献了可实时处理大数据的Impala项目。
  ```

- 包含什么？

  ​	`以下模块主要是针对于2.x之后的版本，因为1.x版本问题较多，使用较少。`

  - 文件模块

    ```shell
    文件系统核心模块(HDFS)：Hadoop Distributed File System
    
    NameNode：集群当中的主节点，主要用于管理集群当中的各种数据
    
    secondaryNameNode：主要能用于hadoop当中元数据信息的辅助管理
    
    DataNode：集群当中的从节点，主要用于存储集群当中的各种数据
    ```

  - 计算模块

    ```shell
    数据计算核心模块(MapReduce)：
    
    ResourceManager：接收用户的计算请求任务，并负责集群的资源分配，以及计算任务的划分
    
    NodeManager：负责执行主节点ResourceManager分配的任务
    ```

  - HA集群

    ```shell
    如果需要搭建HA集群的话，还需要跟Zookeeper搭配使用
    
    JournalNode：元数据信息管理进程，一般都是奇数个
    ```

### Why？

- 特点
  1. 高可靠性：Hadoop底层维护了多个数据副本，即使Hadoop某个计算元素或者存储出现故障，也不会导致数据的丢失。
  2. 高扩展性：在集群间分配任务数据，可方便的扩展数以千计的节点。
  3. 高效性：在MR的思想下，Hadoop是并行工作的，以加快任务处理的速度。
  4. 高容错性：能够自动将失败的任务重新分配。

### How？

#### 环境准备

##### 必须组件的安装

- JDK

  - 第一种，自己下载安装。

    - 下载：`https://www.oracle.com/java/technologies/javase/javase-jdk8-downloads.html`
    - 解压，配置环境变量。
    - 测试

  - 第二种，yum源安装

    ```shell
    yum -y list java*
    yum install -y java-1.8.0-openjdk.x86_64
    java -version
    ```

#### 安装Hadoop

- 下载

  地址：`https://hadoop.apache.org/releases.html`

- 安装

  - 解压jar包；`tar -zxvf xxxx.jar`

  - 配置软连接；`ln -s xxx default`

  - 配置环境变量；

    ```shell
    export HADOOP_HOME=/usr/local/program/hadoop/default
    export HADOOP_CONF_DIR=${HADOOP_HOME}/etc/hadoop
    export PATH=${PATH}:${HADOOP_HOME}/bin:${HADOOP_HOME}/sbin
    ```

  - 测试；`hadoop version`

- 目录解释

```shell
bin目录：存放对Hadoop相关服务（HDFS,YARN）进行操作的脚本

etc目录：Hadoop的配置文件目录，存放Hadoop的配置文件

lib目录：存放Hadoop的本地库（对数据进行压缩解压缩功能）

sbin目录：存放启动或停止Hadoop相关服务的脚本

share目录：存放Hadoop的依赖jar包、文档、和官方案例
```

#### 运行

本地模式、伪分布式模式以及完全分布式模式。

- 本地模式

- 伪分布式

  - 添加slaves文件

  - 修改主要的配置文件

    - core-site.xml

    ```shell
    <configuration>
       <!-- HDFS名称  -->
        <property>
            <name>fs.defaultFS</name>
            <value>hdfs://mengli:9000</value>
            <final>true</final>
        </property>
       <!-- Hadoop所需的所有文件的主目录 -->
        <property>
            <name>hadoop.tmp.dir</name>
            <value>/usr/local/program/hadoop/default/data</value>
        </property>
    </configuration>
    ```
  ```
  
    - hdfs-site.xml
    
    ```shell
    <configuration>
       <!-- 副本数 -->
        <property>
            <name>dfs.replication</name>
            <value>1</value>
        </property>
        <!-- 配置 secondary namenode -->
        <property>
            <name>dfs.namenode.secondary.http-address</name>
            <value>mengli:50090</value>
        </property>
       <!-- 关闭web权限检查 -->
        <property>
            <name>dfs.permissions.enabled</name>
            <value>false</value>
        </property>
         <property>
            # 是否允许namenode将自身进行格式化,在第一次格式化之后设置为false
            <name>dfs.namenode.support.allow.format</name>
            <value>false</value>
        </property>
       <!-- 下面两个参数最好还是明文指定,为以后的数据恢复做打算.
        <property>
            # 配置namenode数据目录
            <name>dfs.namenode.name.dir</name>
            <value>file:///${hadoop.tmp.dir}/dfs/name</value>
        </property>
        <property>
            # 配置datanode数据目录
            <name>dfs.datanode.data.dir</name>
            <value>file:///${hadoop.tmp.dir}/dfs/data</value>
        </property>
       -->
    </configuration>
  ```

  - 配置免密钥登陆

  - 格式化namenode

    ```shell
    bin/hdfs namenode -format
    ```

- 分布式

#### 常用命令的使用

```shell
hadoop fs --help
```